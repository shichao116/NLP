\documentclass[twoside,10pt]{article}
\usepackage{amsmath,amsfonts,amsthm,fullpage}
\usepackage{mymath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}


\begin{document}
\title{\textbf{CS 7650 Problem Set 1: Review of Probability}}
\author{\textbf{Chao Shi, 902604979}}
\maketitle


%----------------------------------------------------------------------------------
\section{Zombie Bob}

\subsubsection*{1.1 Bayes rules}
Denote Bob's likelihood of saying \textit{graagh} as $P(g|B) = 10^{-5}$,
that of Zombie Bob as $P(g|Z) = 0.5$, and accordingly prior probability of Bob
being a zombie was $P(Z) = 10^{-6}$.\\
According to Bayes rules, the posterior is:
\begin{align}
    P(Z|g) = \frac{P(Z)P(g|Z)}{P(Z)P(g|Z) + P(B)P(g|B)} =
    \frac{10^{-6}\times0.5}{10^{-6}\times0.5+10^{-5}\times(1-10^{-6})} =  0.0476
\end{align}

\subsubsection*{1.2 Expected utility}
The posterior probability that it is Bob rather than Zombie is $P(B|g) = 1 - P(Z|g) =
0.9524$. \\
If bob stays, the expected utility is:
\begin{equation}
  u(1) = u(1,Bob)\times P(B|g) + u(1, Zombie)\times P(Z|g) = -20\times0.0476 =
  -0.952 
\end{equation}
If bob runs, the expected utility is
\begin{equation}
  u(2) = u(2,Bob)\times P(B|g) + u(2, Zombie)\times P(Z|g) =
  -1\times0.9524+3\times0.0476 = -0.8096
\end{equation}
Strategy 2 has higher expected utility, so Bob should run.

\subsubsection*{1.3 The chain rule and marginal probabilities}
\begin{equation}
  P(live) = P(live|Z,g)P(Z|g) + P(live|B,g)P(B|g) = 0.5\times0.0476 + 0.9524
  \times1.0 = 0.9762
\end{equation}
\vspace{1cm}




%----------------------------------------------------------------------------------
\section{Necromantic Scrolls}
\subsubsection*{2.1 Bayes rule}
To simplify the notation, denote Anna as A, Barry as B, \textit{abracadabra} as
\textit{a}, \textit{gesundheit} as \textit{g}.\\
According to the problem statement, the prior belief that Anna is the author is
$P(A) = 0.6,\ P(a|A) = 0.005,\ P(a|b) = 0.01$, so we have:
\begin{equation}
  P(A|a) = \frac{P(a|A)P(A)}{P(a|A)P(A)+P(a|B)P(B)} =
  \frac{0.005\times0.6}{0.005\times0.6+0.01\times0.4} = 0.4286 
\end{equation}
\subsubsection*{2.2 Breakevenpoint}
First we calculate posterior of author being Barry:
\begin{align}
  &P(B|a) = \frac{P(a|B)P(B)}{P(a|B)P(B) + P(a|A)P(A)}\\
  &\Rightarrow \frac{P(A|a)}{P(B|a)} = \frac{P(a|A)P(A)}{P(a|B)P(B)} =
  \frac{P(a|A)P(A)}{P(a|B)(1-P(A))} =\frac{0.005\times P(A)}{0.01\times(1-P(A))}
\end{align}
What we want is the ratio to be 1, so let
\begin{equation}
  \frac{0.005\times P(A)}{0.01\times(1-P(A))} = 1 \Rightarrow P(A) = \frac{1}{3}
\end{equation}
\subsubsection*{2.3 Multiple words}
\begin{itemize}
  \item Denote the result in problem statement as event E. In order to calculate
  thte posterior belief P(A|E), we need to first calculate $P(E|A)$ and
  $P(E|B)$. For each
  word w, there're three cases: 1) w=\textit{a}; 2) w=\textit{g}; 3) otherwise.
  Denote the corresponding probabilities: 1) $P(a|A) = 0.005, P(a|B) = 0.01$;
  $P(g|A) = 0.006,\ P(g|B) = 0.001$;
  $P(o|A) = 1 - P(a|A) - P(g|A) = 0.989,\ P(o|B) = 1 - P(a|B) - P(g|B)=0.989$. The first case happened twice, the
  second once, and thus the third 97 times. Then $P(E|A)$ can be described by
  multinomial distribution as following:
  \begin{align} 
    P(E|A) &=
    \frac{100!}{2!\times1!\times97!}\times0.005^{2}\times0.006\times0.989^{97} =
    0.0249\\
    P(E|B) &=
    \frac{100!}{2!\times1!\times97!}\times0.01^{2}\times0.001\times0.989^{97} =
    0.0166
  \end{align}
  Then the posterior can be calculated as:
  \begin{equation}
    P(A|E) = \frac{P(E|A)P(A)}{P(E|A)P(A)+P(E|B)P(B)} =
    \frac{0.0249\times0.5}{0.0249\times0.5+0.0166\times0.5} = 0.6 
  \end{equation}

\item From the caculation above, we can see that we only need to know the
  probability of a word not being \textit{abracadabra} or \textit{gesundheit},
  which can be calculated by the frequencies of these two words.\\ 
  So we \textbf{DO NOT}
  need any more information about the other 97 words.
\end{itemize}
%----------------------------------------------------------------------------------
\section{Sentence lengths}
\subsubsection*{3.1 Maximum likelihood estimation}
Log likelihood:
\begin{equation}
  \log P(l_{1:N}|\lambda) = \prod_{n=0}^{N}\lambda^n(1-\lambda) =
  \sum_{n=0}^{N} (\log\lambda^n + \log(1-\lambda)) = \frac{N(N+1)}{2}\log\lambda
  + N\log(1-\lambda)
\end{equation}
Let the derivative of the log likelihood w.r.t $\lambda$ be zero and we will
have the value of $\lambda$:
\begin{equation}
  \frac{\partial\log p(l_{1:N}|\lambda)}{\partial\lambda} =
  \frac{N(N+1)}{2\lambda} - \frac{N}{1-\lambda} = 0\Rightarrow
  \lambda=\frac{N+1}{N+3}
\end{equation}

\subsubsection*{3.2 Expectations}
\begin{itemize}
  \item Expectation of sentence length:
    \begin{equation}
      E[l] = \sum_{l=0}^{\infty}l\lambda^l(1-\lambda) =
    (1-\lambda)\sum_{l=1}^{\infty}l\lambda^l = \frac{\lambda}{1-\lambda} 
  \end{equation}
  \item Modal sentence length.
    \begin{equation}
      \frac{\partial P(l)}{\partial l} = l\lambda^{n-1}-(l+1)\lambda^l =
      0\Rightarrow l = \frac{\lambda}{1-\lambda}
    \end{equation}
    So the modal sentence length is either 
  \item Skewed distribution
\end{itemize}

%----------------------------------------------------------------------------------
\section{Part-of-speech tagging accuracy}

\subsubsection*{4.1} The probability of tagging a word correctly is $1 - 0.1 =
0.9$, and tagging each word correctly or not is IID. So for $n = 5$, the
probability is simply $P = 0.9^5 = 0.59$

\subsubsection*{4.2} Since the number of verbs is less than or equal to the
number of all words in a sentence(much less most of the time actually), the
probability of Gregory's tagger to make errors is of course lower than that of 
Felicia's tagger. So Gregory's tagger will get more sentences completely
correct.


\end{document}
