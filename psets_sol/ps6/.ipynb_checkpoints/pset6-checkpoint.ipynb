{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6. Distributed Embeddings and Semi-Supervised Learning #\n",
    "\n",
    "In this assignment, you will use singular value decomposition (SVD) as well as Word2Vec to learn about lexical semantics. You will have to work with \"big\" data\n",
    "- big enough that you will have to think carefully about speed and memory. Of particular importance will **sparse** matrix\n",
    "representations of your data. For this problem you will be submitting pdf version of ipython with outputs along with original ipython. Some particular functions and classes you might need:\n",
    "\n",
    "- [scipy.sparse.csr_matrix](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) - matrix in compressed sparse row format\n",
    "- [scipy.sparse.diags](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.diags.html) - method for creating sparse diagonal matrices\n",
    "- [diagonal()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html) - get the diagonal of a matrix\n",
    "- [sklearn.preprocessing.normalize](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) - efficiently normalize sparse matrices\n",
    "- [scipy.sparse.csr_matrix.asfptype()](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.asfptype.html) - upcast matrix to a floating point format\n",
    "- [scipy.cluster.vq.kmeans2](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html#scipy.cluster.vq.kmeans2) - Classify a set of observations into k clusters using the k-means algorithm\n",
    "- [numpy.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) - returns the indices that would sort an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import hstack, diags, csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab --no-import-all inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csv2csr(filename):\n",
    "    word = []\n",
    "    context = []\n",
    "    count = []\n",
    "    with open(filename,'rb') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            word.append(int(row[0]))\n",
    "            context.append(int(row[1]))\n",
    "            count.append(int(row[2]))\n",
    "    return csr_matrix((count,(word,context)))\n",
    "\n",
    "def readVocab(filename):\n",
    "    vocab = []\n",
    "    with open(filename,'rb') as vocabfile:\n",
    "        for line in vocabfile:\n",
    "            vocab.append(line.split()[0])\n",
    "    index = dict(zip(range(0,len(vocab)),vocab)) #from numbers to words\n",
    "    inv_index = {j:i for i,j in index.items()} #from words to numbers\n",
    "    return index,inv_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data ##\n",
    "\n",
    "Call **C=proj4_starter.csv2csr('doc_trips.csv')** to load a sparse matrix $C$ of a word-document counts. The cell $c[i,j]$ should \n",
    "hold the count of word $i$ in document $j$.\n",
    "\n",
    "Call **idx, iidx=proj4_starter.readVocab('vocab.10k')** to load the vocabulary. You get two **dict** objects, mapping between words \n",
    "and indices in the matrix $C$. In **C[iidx['Obama'],:]**, you have the document counts for the word *Obama*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 1000000)\n"
     ]
    }
   ],
   "source": [
    "C = csv2csr('doc_trips.csv')\n",
    "idx, iidx = readVocab('vocab.10k')\n",
    "print C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9993x1000000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 7733607 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cosine Similarity ##\n",
    "\n",
    "The *cosine similarity* of two vectors $u$ and $v$ is defined as \n",
    "$$\\frac{\\sum_{i}u_iv_i}{\\sqrt{\\sum_iu_i^2\\sum_iv_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2a** Consider the words *coffee, play, crazy, facebook*, and *hermana* (Spanish for *sister*). For each of them, find the 10 most similar words according to cosine similarity of the rows in $C$. (2 points)\n",
    "\n",
    "**Hint** The size of the vocabulary is nearly 10,000 words. You do not want to compute and store the entire $10K\\times 10K$ matrix \n",
    "of cosine similarities. Rather, you want to compute them on demand for a given row of the matrix. You may also want to do some\n",
    "precomputation to take care of denominator in advance. Whatever you do, don't lose the sparsity of $C$, or you will not be able \n",
    "to store it.\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words I get are *facebook page on twitter deleted instagram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is the word list\n",
    "word_list = ['coffee','play','crazy','facebook','hermana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeRow(x):\n",
    "    '''\n",
    "    Normalize each row of x\n",
    "    '''\n",
    "    return diags(np.array(1./(1e-6+np.sqrt(x.multiply(x).sum(axis=1))))[:,0],0) * x\n",
    "\n",
    "def computeCosSimPerWord(word_idx, x):\n",
    "    '''\n",
    "    For a given data matrix, compute cosine similarity between the word with index \"word_index\" and all words \n",
    "    (including itsself)\n",
    "    \n",
    "    Should return a 1-D np.array, not a matrix.\n",
    "    '''\n",
    "    # x should be normalizedC below\n",
    "    return x.dot(x[word_idx,:].T).toarray()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedC = normalizeRow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSimilarWords  is used to print the top 10 similar words to a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printSimilarWords(x, word_list, sim_func, vocab=idx, ivocab=iidx):\n",
    "    for word in word_list:\n",
    "        print word, ':', \n",
    "        word_idx = ivocab[word]\n",
    "        sim_idx = np.argsort(-sim_func(word_idx, x))[:10]\n",
    "        for word2_idx in sim_idx:\n",
    "            print vocab[word2_idx],\n",
    "        print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee mug shop starbucks drinking drink cup cups and large \n",
      "play : play to games game the and with soccer i . \n",
      "crazy : crazy 's how that it shit is drives i . \n",
      "facebook : facebook page on twitter deleted instagram status compra post whatsapp \n",
      "hermana : hermana mi concha la y de tu con regalo que \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedC, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2b ** Come up with five words of your own that you think might be interesting, and list the top 10 most similar for each. Try to choose a few different types of words, such as verbs, adjectives, names, emotions, abbreviations, or alternative spellings. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "additional_word_list = ['computer', 'google', 'science', 'math', 'physics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer : computer lab my screen transfer gates staring virus science on \n",
      "google : google search image compra global searching autocorrect youtube according facebook \n",
      "science : science maths physical earth rocket computer english math study balance \n",
      "math : math homework teacher class test problems study exam solve subject \n",
      "physics : physics maths lab tests further subject assignment reaction british kanina \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedC, additional_word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Co-occurence ##\n",
    "\n",
    "Compute the document co-occurence matrix $D$, where $d_{i,j}$ is the probability $P(w_j|w_i)$ that word $j$ appears in a tweet, \n",
    "given that word $i$ appears. To do this, first compute the co-occurence counts $CC^\\top$. Substract the diagonal, then normalize \n",
    "each row. \n",
    "\n",
    "Note: it is possible to smooth this probability, but if you naively add some number to the matrix, you will lose sparsity \n",
    "and memory will blow up. You can do it unsmoothed. However, smoothing is not required here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3** For each of the 10 examples above (my five words and your five words), find the 10 most similar words according to cosine similarity of the rows of $D$. (2 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the 5 words I get are *facebook instagram twitter tv youtube*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeCooccurMatrix(C):\n",
    "    '''\n",
    "    Compute the co-occurence matrix D\n",
    "    '''\n",
    "    D = C.dot(C.T)\n",
    "    D = D - diags(D.diagonal(),0)\n",
    "    '''\n",
    "    coeff_norms = np.array(np.sqrt(C.multiply(C).sum(axis=1)))[:,0]\n",
    "    for i in range(D.nnz):\n",
    "        print i\n",
    "        row = D.nonzero()[0][i]\n",
    "        col = D.nonzero()[1][i]\n",
    "        coeff = np.sqrt(coeff_norms[row]*coeff_norms[col])\n",
    "        D[row,col] = D[row,col]/coeff\n",
    "    '''\n",
    "    return D\n",
    "\n",
    "D = computeCooccurMatrix(C)\n",
    "normalizedD = normalizeRow(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = word_list + additional_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee new today drinking getting food after day eating water \n",
      "play : play down be back pass show beat them up run \n",
      "crazy : crazy weird sad not stupid damn actually cool about over \n",
      "facebook : facebook instagram twitter tv youtube tumblr 100 note ex insta \n",
      "hermana : hermana abuela hermano vieja novio novia viejo padre familia corazon \n",
      "computer : computer phone mind body family own arm head cat husband \n",
      "google : google online okay true 100% super alright wow very oh \n",
      "science : science free record red gold new big form line football \n",
      "math : math seriously lost today exam damn completely literally test paper \n",
      "physics : physics new after book dance test kid with time today \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(normalizedD, word_list, sim_func=computeCosSimPerWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Semantic Analysis ##\n",
    "\n",
    "Perform truncated SVD (**scipy.sparse.linalg.svds**) to obtain $USV^\\top\\approx C$ using $K=10$. Each row vector $u_i$\n",
    "is a description of the word $i$. You can compute similarity between pairs of words using the squared Euclidean norm \n",
    "$\\|u_i-u_j\\|^2_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(a)** For each of the 10 examples above, find the 10 most similar words according to squared Euclidean distance in $U$. (3 points)\n",
    "\n",
    "**Sanity check** For *facebook*, the top 5 words are *facebook ex harry calls snap *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeEuclidDist(word_idx, U,with_S = False):\n",
    "    '''\n",
    "    Compute the Euclid distance bwteen word with index \"word_idx\" and all words \n",
    "    (including itself)\n",
    "    \n",
    "    Args:\n",
    "      word_idx - the word index\n",
    "      U - latent representation of words\n",
    "    Return:\n",
    "        Euclidean distance from representation of word_idx to all words\n",
    "    '''\n",
    "    dis = np.sum((U - U[word_idx,:])**2,axis=1)\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Once you finish the function computeEuclidDist, run the following code directly to print results\n",
    "'''\n",
    "Cfp= C.asfptype()\n",
    "U = svds(Cfp, 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee drinking paper cat eating apparently turned short dress months \n",
      "play : play looking everyone anyone another start coming use enough away \n",
      "crazy : crazy gone stupid sad cool yet funny perfect must once \n",
      "facebook : facebook ex harry calls snap instagram ...... uh 18 fav \n",
      "hermana : hermana unas minutos colegio toca novia verte gana juego unos \n",
      "computer : computer productive tie swim paint cancer adult reaction wide dry \n",
      "google : google adam mrs download ryan nick hero speaking haters cash \n",
      "science : science wood silver chapter range political wing rate injured setting \n",
      "math : math emotional stayed annoyed glasses stopped throat oops studying grew \n",
      "physics : physics blunts nine washed beyonce rolled pounds dip 1% shoulders \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(b) ** Now compute the same SVD with $K=50$, and again find the 10 most similar words according to Euclidean distance $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "U = svds(Cfp,50)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee tea wine four gas midnight starbucks service wedding six \n",
      "play : play hang run chill we're agree nobody kids fight games \n",
      "crazy : crazy weird stupid actually af dumb serious dead ugly dude \n",
      "facebook : facebook instagram focus ig netflix earth floor fb tl list \n",
      "hermana : hermana sonrisa llevo plata jajajaj realidad arriba jajaj vuelta novia \n",
      "computer : computer forehead ankle toe toes battery shoulder shoulders plate whilst \n",
      "google : google dvd flappy drawing bbc gd spin vid mobile replay \n",
      "science : science indian silver ukraine agent ops 140 bjp alabama oklahoma \n",
      "math : math research chemistry physics doo frozen tech lab appointment ...? \n",
      "physics : physics pong bd mesaj 150 cricket ahahahaha smfh tfios lawrence \n"
     ]
    }
   ],
   "source": [
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 4(c) ** Now compute the SVD of the matrix $\\mathbf{D}$, using with $K = 10$, and $K = 50$. Report \n",
    "the most similar words to each of the example words according to Euclidean distance in $U$. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee paper complete chinese turning workout ordered hour perfectly cutting \n",
      "play : play animals order fill hyped they halfway out speaking happening \n",
      "crazy : crazy still worked checked yesterday started more might opinion surprised \n",
      "facebook : facebook anime gossip 10x ex chocolate lips loose calls flappy \n",
      "hermana : hermana cualquier huevo abuela #buenasnoches poco las mis durmiendo vueltas \n",
      "computer : computer daughter break certain growing yogurt husband eating shift belly \n",
      "google : google online jackie super ab goo abs facts safe 9:30 \n",
      "science : science red utd silver bill stretch runs walker landed indian \n",
      "math : math typing timeline fml starving glasses addiction ink ruined ruin \n",
      "physics : physics teacher blunts beyonce bone sex #smh finding sunburn closet \n"
     ]
    }
   ],
   "source": [
    "# your code here for K = 10\n",
    "Dfp = normalizedD.asfptype()\n",
    "U = svds(Dfp, 10)[0]\n",
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee tea six food night today half sugar new miles \n",
      "play : play pass run i'm win beat other him back be \n",
      "crazy : crazy nasty cool funny lame true fast pretty scary not \n",
      "facebook : facebook instagram twitter ig youtube netflix focus based tv tumblr \n",
      "hermana : hermana novia abuela vieja hermano reina sonrisa familia padre amiga \n",
      "computer : computer laptop phone screen account arm leg shoulder license ankle \n",
      "google : google 100% reading 100 online ok too correct about an \n",
      "science : science david paul code football an health challenge christ career \n",
      "math : math exam literally english lost now test essay seriously today \n",
      "physics : physics nine new bell soccer basketball writing disney rose gaga \n"
     ]
    }
   ],
   "source": [
    "# your code here for K = 50\n",
    "Dfp = normalizedD.asfptype()\n",
    "U = svds(Dfp, 50)[0]\n",
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee tea food pizza drink new half green ice all \n",
      "play : play be run beat win start down up drive come \n",
      "crazy : crazy actually about dude scary cool not too legit pretty \n",
      "facebook : facebook twitter instagram tv youtube tumblr ig social focus based \n",
      "hermana : hermana hermano abuela vieja novio reina padre favorito novia corazon \n",
      "computer : computer phone screen laptop flight arm account toe list ankle \n",
      "google : google an ok too show as haha 100 lol now \n",
      "science : science water new an code joe challenge meat american free \n",
      "math : math now test chemistry period study literally all apparently lost \n",
      "physics : physics math gas now actually bell green near basketball new \n"
     ]
    }
   ],
   "source": [
    "# optionally, try K = 100 and see if it's even better\n",
    "Dfp = normalizedD.asfptype()\n",
    "U = svds(Dfp, 100)[0]\n",
    "printSimilarWords(U, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local Context ##\n",
    "\n",
    "Local context captures the frequency with which words appear in each others’ immediate context. We have provided a CSV file (succ_trips_50k.csv)  in which each line contains a triple $\\langle x,y,z\\rangle$, \n",
    "where $x$ and $y$ are term IDs and $z$ is the count of times where $y$ immediately follows $x$ . \n",
    "The vocabulary has now increased to 50K words. There is an associated vocabulary file, **vocab.50k**.\n",
    "\n",
    "**Deliverable 5a **\n",
    "Build a sparse matrix $\\mathbf{E}$ from these triples. Normalize the rows of $\\mathbf{E}$, such that $e_{i,j}=\\frac{n(i,j)}{n(i)}$, the probability of seeing word $j$ given that you have just seen word $i$. \n",
    "Now form a matrix $\\mathbf{F} = [\\mathbf{E}~ \\mathbf{E}’]$ by horizontally concatenating the normalized matrix $\\mathbf{E}$. You will perform sparse singular value decomposition on $\\mathbf{F}$. (2 points)\n",
    "\n",
    "**Hint** make sure you are using a sparsity-preserving operation to combine E and E'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_50, iidx_50 = readVocab('vocab.50k')\n",
    "E = csv2csr('succ_trips_50k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def constructF(E):\n",
    "    '''\n",
    "    Finish the following code to construct F from E\n",
    "    '''\n",
    "    nE = diags(1./(1e-6+np.array(E.sum(axis=1)))[:,0],0)*E\n",
    "    Enorm = sp.hstack((nE,nE.T))\n",
    "    return Enorm\n",
    "    \n",
    "F = constructF(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 5b ** For $K = 10$ and $K = 50$ compute the top 10 synonyms for each of your ten words. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee low common line hill learning golf bestfriend baseball speech \n",
      "play : play through keep live black show white between eat red \n",
      "crazy : crazy times card monday funny stupid fun fast died slow \n",
      "facebook : facebook touchdown tasha hooked cnn drugged 99% giggs carrie alcohol \n",
      "hermana : hermana hermano ando aun negro vivimos gordo duran tantos sera \n",
      "computer : computer reaction camera reserve nation truck bank steel cat license \n",
      "google : google holla carter elvis clearly megan don't. norman ashley acid \n",
      "science : science deliver binge bill guardian hip rate header wing washed \n",
      "math : math hiking lacking bitchy gps contest blair randle sex bolton \n",
      "physics : physics cba lallana cuts commentators diamonds technique hull comm collecting \n"
     ]
    }
   ],
   "source": [
    "# K = 10\n",
    "U_f,S_f,V_f = svds(F, 10)\n",
    "# note that we have to insert the new vocabulary as an optional argument\n",
    "printSimilarWords(U_f, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U),vocab=idx_50,ivocab = iidx_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee grace selfies milk dicks posts yogurt wage fathers blunts \n",
      "play : play comes fly drive hang skip perform kick goes talks \n",
      "crazy : crazy funny dead dope scary monday annoying far creepy cancelled \n",
      "facebook : facebook tumblr twitter youtube saturdays messin instagram anatomy valentines announced \n",
      "hermana : hermana tomo canta siendo haciendo papa llora avete escuchando hepimiz \n",
      "computer : computer laptop soul mouth wrist room desk shoulder boyfriends husband \n",
      "google : google comment emoji skipping bak yaya id scout fair floyd \n",
      "science : science england kentucky various spain training five april serving effect \n",
      "math : math gfs pad bio tax tits blood manager shift business \n",
      "physics : physics english drivers theory buds exam italian sneakers pad lining \n"
     ]
    }
   ],
   "source": [
    "# your code for K = 50 here\n",
    "U_f,S_f,V_f = svds(F, 50)\n",
    "# note that we have to insert the new vocabulary as an optional argument\n",
    "printSimilarWords(U_f, word_list, sim_func=lambda word_idx, U : -computeEuclidDist(word_idx,U),vocab=idx_50,ivocab = iidx_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5c ** Overall, which set of synonyms looks best to you? \n",
    "Count how many of the top 5 synonyms for *coffee* and   *crazy*\n",
    "have the same majority part of speech (e.g., *play* is a verb) as the cue word.\n",
    "Use the tagset from the [Twitter POS paper](http://www.cc.gatech.edu/~jeisenst/papers/acl2012pos.pdf). Does local context or document context do better at matching the POS of the cue words? Why? (Consider first context provided by wordnet as majority POS). (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Your answer here)*: \n",
    "- Overall, the set of synonyms of \"facebook\" looks best to me: facebook, tumblr, twitter, youtube are all very similar words w.r.t facebook.\n",
    "- coffee: grace selfies milk dicks posts. All top five have the same POS as coffee: noun.\n",
    "- crazy: funny dead dope scary monday. Among top 5 synonyms of crazy, funny, dead, and scary have the same POS as crazy: adjective.\n",
    "\n",
    "Local context does better at matching the POS of cue words. Because local context tend to select words that can be mutually replaced at the same place(at least syntactically),which means these words are very likely has the same POS. However, document context tend to select words that are topic-related, such as \"dog and bark\", \"cat and scratch\" etc, apparently they don't have the same POS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec ##\n",
    "As mentioned in the class Word2Vec are distributed continuous vector representations  for words.\n",
    "In this part, you will be building and training Word2Vec module keeping in mind various hyper parameters such as min_count, window_size etc.(10 points)\n",
    "You will be using Gensim http://radimrehurek.com/gensim/ to train your model.\n",
    "\n",
    "- For instructions on how to install gensim, see [here](https://radimrehurek.com/gensim/install.html). \n",
    "- I strongly recommend that you test your install by downloading the [source](http://pypi.python.org/pypi/gensim) and running ```python setup.py test```\n",
    "- For a tutorial on how to use gensim for word embeddings, see [here](http://rare-technologies.com/word2vec-tutorial/)\n",
    "- It is recommended that you have a working c compiler, so that the much faster cythonized gensim can run. This is transparent to you, but does require that you have a c compiler installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "        print \"Opening the file...\"\n",
    "        X_train = []\n",
    "        f = open(filename,'r')\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            sentence = []\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                sentence = word_tokenize(line)\n",
    "            except:\n",
    "                pass\n",
    "            if(len(sentence) > 2):\n",
    "                count =count+1\n",
    "                X_train.append(array(sentence))\n",
    "        print \"File successfully read\"\n",
    "        print count , \"sentences\"\n",
    "        f.close()\n",
    "        return array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the file...\n",
      "File successfully read\n",
      "17392 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = read_data('wiki.train.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anarchism', 'is', 'a', 'political', 'philosophy', 'that',\n",
       "       'advocates', 'stateless', 'societies', 'often', 'defined', 'as',\n",
       "       'self-governed', 'voluntary', 'institutions', ',', 'but', 'that',\n",
       "       'several', 'authors', 'have', 'defined', 'as', 'more', 'specific',\n",
       "       'institutions', 'based', 'on', 'non-hierarchical', 'free',\n",
       "       'associations', '.', 'Anarchism', 'holds', 'the', 'state', 'to',\n",
       "       'be', 'undesirable', ',', 'unnecessary', ',', 'or', 'harmful', '.',\n",
       "       'While', 'anti-statism', 'is', 'central', ',', 'anarchism',\n",
       "       'entails', 'opposing', 'authority', 'or', 'hierarchical',\n",
       "       'organisation', 'in', 'the', 'conduct', 'of', 'human', 'relations',\n",
       "       ',', 'including', ',', 'but', 'not', 'limited', 'to', ',', 'the',\n",
       "       'state', 'system', '.'], \n",
       "      dtype='|S16')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build and train model you will be using [Gensim](http://radimrehurek.com/gensim/). \n",
    "\n",
    "This [tutorial](https://radimrehurek.com/gensim/models/word2vec.html) may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6a**: Build a model with embedding size = 10, and build a vocabulary from the data, with min_count = 10. Print the vocabulary size, which should be between 9000 and 10000. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9911\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import gensim\n",
    "model = gensim.models.Word2Vec(sentences,min_count=10,size=8,iter=3) #size = 10 doesn't meet the requirement, however, size=8 does\n",
    "print len(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6b**: Train your model for one iteration, and report the similarity between the words (\"cat\" and \"dog\"), (\"night\" and \"day\"), and (\"can\" and \"fish\"). (2 points)\n",
    "\n",
    "**Hint**: before training on the whole dataset, train on sentences[0:5] to make sure it works and isn't too slow. On my laptop, one iteration takes 2-3 seconds. \n",
    "\n",
    "**Sanity check** the similarity I get between \"cat\" and \"dog\" is 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312399"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between 'cat' and 'dog': 0.172515839666\n",
      "similarity between 'night' and 'day': 0.854251690713\n",
      "similarity between 'can' and 'fish': 0.24742204616\n"
     ]
    }
   ],
   "source": [
    "print \"similarity between 'cat' and 'dog':\",model.similarity('cat','dog')\n",
    "print \"similarity between 'night' and 'day':\",model.similarity('night','day')\n",
    "print \"similarity between 'can' and 'fish':\",model.similarity('can','fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6c** Modify the code below to compare the performance of different embedding sizes. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainMod(model,sentences,max_its=10):\n",
    "    # your code to build the vocabulary\n",
    "    scores = []\n",
    "    times = []\n",
    "    for _ in xrange(max_its):\n",
    "        # your code to train the model\n",
    "        model.train(sentences)\n",
    "        scores.append(sum(model.score(sentences)))\n",
    "        times.append(model.total_train_time)\n",
    "    return scores,times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with 20\n",
      "done with 40\n",
      "done with 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fcab15dee50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEVCAYAAAAo63jjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ/4/fGI7eHYJnsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcab2b3e9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_k = [20,40,100] # embedding sizes\n",
    "models = []\n",
    "for k in all_k:\n",
    "    model = gensim.models.Word2Vec(sentences, min_count=10, size=k) #your code to create a model here\n",
    "    models.append(model)\n",
    "    scores,times = trainMod(model,sentences)\n",
    "    plt.plot(times,scores,'o-');\n",
    "    model.save('model-%d'%(k))\n",
    "    print 'done with',k\n",
    "plt.legend(['k=%d'%(k) for k in all_k],loc='lower right');\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6d** Use the ```most_similar``` function to find similar words to the items in your word_list which are in the vocabulary for your word2vec models. Compare these most similar words with the outputs of the methods tried earlier in the assignment. You may train the word2vec models for longer if you wish, or change any of the parameters. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 20\n",
      "coffee : [('Taiwan', 0.7958792448043823), ('Wireless', 0.7918726205825806), ('malaria', 0.780410885810852), ('Lanka', 0.7728040218353271), ('Bulgaria', 0.7617757320404053), ('tobacco', 0.7575591206550598), ('vegetables', 0.7532111406326294), ('corn', 0.7470266819000244), ('Herat', 0.74656081199646), ('timber', 0.7395137548446655)]   \n",
      "\n",
      "play : [('series', 0.814768373966217), ('match', 0.8064485192298889), ('prize', 0.7955950498580933), ('title', 0.7895134687423706), ('publishing', 0.7826794385910034), ('Ashes', 0.7821651101112366), ('tournament', 0.7790948152542114), ('obituary', 0.7631993889808655), ('played', 0.7575876712799072), ('Poirot', 0.7538279294967651)]   \n",
      "\n",
      "computer : [('device', 0.8768742084503174), ('digital', 0.8441665172576904), ('video', 0.8429996967315674), ('software', 0.835493266582489), ('operating', 0.8238778114318848), ('computers', 0.8059135675430298), ('setting', 0.8002673983573914), ('encoding', 0.7911180257797241), ('graphics', 0.7854411602020264), ('online', 0.7844991087913513)]   \n",
      "\n",
      "science : [('fiction', 0.830358624458313), ('archaeology', 0.8115688562393188), ('philosophical', 0.8089278340339661), ('metaphysical', 0.8003947734832764), ('Bach', 0.8002785444259644), ('concepts', 0.7926318645477295), ('conceptual', 0.7906367182731628), ('imagery', 0.7893315553665161), ('agrarianism', 0.7883901000022888), ('mainstream', 0.7814579010009766)]   \n",
      "\n",
      "physics : [('psychology', 0.9006938934326172), ('theoretical', 0.9001275897026062), ('ethics', 0.78456711769104), ('mathematics', 0.7766550779342651), ('science', 0.7726116180419922), ('mechanics', 0.7668485641479492), ('linguistics', 0.7596691846847534), ('sociology', 0.7571589350700378), ('archaeology', 0.7501922845840454), ('ethical', 0.7490352392196655)]   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k= 40\n",
      "coffee : [('malaria', 0.6738451719284058), ('tobacco', 0.6733990907669067), ('vegetables', 0.6486682295799255), ('timber', 0.6441667675971985), ('corn', 0.640870213508606), ('potatoes', 0.6261817812919617), ('raw', 0.6226439476013184), ('Taiwan', 0.6158177852630615), ('Lanka', 0.6062631011009216), ('rice', 0.6054402589797974)]   \n",
      "\n",
      "play : [('series', 0.7305195331573486), ('Marple', 0.6745297312736511), ('match', 0.6741989254951477), ('publishing', 0.6584540605545044), ('novel', 0.6568641066551208), ('played', 0.6397168040275574), ('plays', 0.636585533618927), ('Poirot', 0.630850076675415), ('Ashes', 0.629186749458313), ('title', 0.6256369352340698)]   \n",
      "\n",
      "computer : [('video', 0.7436246871948242), ('digital', 0.7396311163902283), ('computers', 0.7314355373382568), ('programming', 0.7255967855453491), ('graphics', 0.7180377840995789), ('interface', 0.7032506465911865), ('software', 0.7007974982261658), ('device', 0.6920803189277649), ('processor', 0.6888123750686646), ('technology', 0.6831920146942139)]   \n",
      "\n",
      "science : [('psychology', 0.7245497703552246), ('fiction', 0.7182079553604126), ('scientific', 0.7070208787918091), ('archaeology', 0.7031823396682739), ('theoretical', 0.6963920593261719), ('alchemy', 0.6869875192642212), ('topic', 0.6781374216079712), ('historical', 0.6780840754508972), ('spirituality', 0.6628594398498535), ('pioneered', 0.6425073146820068)]   \n",
      "\n",
      "physics : [('theoretical', 0.809085488319397), ('mechanics', 0.7378420829772949), ('psychology', 0.7015141248703003), ('mathematics', 0.6989496946334839), ('sciences', 0.6703993082046509), ('ethics', 0.6527222990989685), ('Einstein', 0.6526251435279846), ('relativity', 0.6525771617889404), ('Schr\\xc3\\xb6dinger', 0.644551157951355), ('biology', 0.6283833980560303)]   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k= 100\n",
      "coffee : [('tobacco', 0.5553410649299622), ('timber', 0.5436915159225464), ('vegetables', 0.5178430676460266), ('rice', 0.5157457590103149), ('potatoes', 0.49840834736824036), ('malaria', 0.4921170473098755), ('fruits', 0.48390480875968933), ('corn', 0.47276780009269714), ('manure', 0.4561580717563629), ('petroleum', 0.45240479707717896)]   \n",
      "\n",
      "play : [('played', 0.5581350922584534), ('Karpov', 0.5245917439460754), ('Poirot', 0.5183738470077515), ('plays', 0.5026705861091614), ('novel', 0.48235440254211426), ('win', 0.47996985912323), ('Marple', 0.4557644724845886), ('match', 0.4463844299316406), ('tournament', 0.43751102685928345), ('retirement', 0.43676382303237915)]   \n",
      "\n",
      "computer : [('computing', 0.5726771354675293), ('programming', 0.5675442814826965), ('programmer', 0.5454590320587158), ('software', 0.5393569469451904), ('wireless', 0.5174508094787598), ('processor', 0.5117985606193542), ('Computer', 0.5063188672065735), ('graphics', 0.5036603808403015), ('programs', 0.5032244920730591), ('machine', 0.5004247426986694)]   \n",
      "\n",
      "science : [('fiction', 0.5940546989440918), ('archaeology', 0.5462543964385986), ('physics', 0.5333342552185059), ('alternate', 0.4743742346763611), ('mathematics', 0.47082364559173584), ('philosophical', 0.47044384479522705), ('interdisciplinary', 0.4582461714744568), ('chemistry', 0.4547930955886841), ('fantasy', 0.4519202709197998), ('logic', 0.4513832926750183)]   \n",
      "\n",
      "physics : [('theoretical', 0.632990837097168), ('mathematics', 0.5696405172348022), ('chemistry', 0.5597614049911499), ('sciences', 0.5539968609809875), ('science', 0.5333341956138611), ('metaphysics', 0.4907146692276001), ('psychology', 0.48131418228149414), ('Schr\\xc3\\xb6dinger', 0.4695616662502289), ('scientific', 0.4640956223011017), ('Recently', 0.45634907484054565)]   \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for i,model in enumerate(models):\n",
    "    print \"k=\",all_k[i]\n",
    "    for word in word_list:\n",
    "        if word in model.vocab:\n",
    "            print word,':',model.most_similar(word),'  \\n'\n",
    "        else:\n",
    "            continue\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(your explanation here)\n",
    "- As wee see as k increases, the most similar words found makes more and more sense in terms of topic-relativeness, e.g. the word 'Taiwan' ranked No.1 in top 10 most similar words of coffee when k = 20, which dropped to No.8 when k = 40, and out of top 10 when k = 100. This is expected because as you include more and more neighboring, you can get more and more context information.\n",
    "- In general it works better than LSA and Local Context using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the claims about word2vec word embeddings is that they can solve analogy problems, like \"man:woman::king:queen\"\n",
    "\n",
    "[This tutorial](http://rare-technologies.com/word2vec-tutorial/) contains example code showing how to use word vectors to try to solve analogies.\n",
    "\n",
    "**Deliverable 6e** Using the three models trained in the previous deliverable, see if each can solve the analogies man:woman::king:queen and architect:building::painter:painting. Invent two more analogies, and see which of the three models can solve them. You can Google analogies, but you may have to search for a while to find analogies where all four elements are in the vocabulary. You can also retrain the models with a larger vocabulary if you want. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to 6e:\n",
    "- No apparently it does not solve analogies very well, as you can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 20\n",
      "man:woman::king:? [('mistaken', 0.8264424800872803), ('witness', 0.8144932985305786), ('protagonist', 0.7855388522148132), ('creature', 0.779259204864502), ('impression', 0.7787606716156006), ('boy', 0.7532042860984802), ('dinner', 0.7517200112342834), ('good', 0.7417856454849243), ('chance', 0.7412746548652649), ('opportunity', 0.7382452487945557)] \n",
      "\n",
      "architect:building::painter:? [('system', 0.7445379495620728), ('monument', 0.725237250328064), ('road', 0.7185670137405396), ('sport', 0.7180393934249878), ('harbour', 0.7033640146255493), ('diocese', 0.6924789547920227), ('central', 0.6686609983444214), ('Chaco', 0.6672202348709106), ('hub', 0.6605966091156006), ('Much', 0.6603871583938599)] \n",
      "\n",
      "science:physics::planet:? [('theoretical', 0.8495887517929077), ('mysticism', 0.834299623966217), ('psychology', 0.833200216293335), ('archaeology', 0.788750946521759), ('entities', 0.7664759159088135), ('philosophical', 0.7614248991012573), ('sociology', 0.7585877180099487), ('exploring', 0.7548397779464722), ('scientific', 0.7495036125183105), ('racism', 0.749194860458374)] \n",
      "\n",
      "Google:searching::computer:? [('mad', 0.6590766310691833), ('wrestling', 0.6559967398643494), ('recognised', 0.6430768966674805), ('police', 0.642218291759491), ('crimes', 0.6314494609832764), ('athletes', 0.6271253824234009), ('occasional', 0.6177042722702026), ('lengthy', 0.6171760559082031), ('husband', 0.6140956878662109), ('operators', 0.6069259643554688)] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k= 40\n",
      "man:woman::king:? [('good', 0.6666606664657593), ('person', 0.6576420068740845), ('happy', 0.6447070837020874), ('mankind', 0.6083214282989502), ('young', 0.6080859899520874), ('fellow', 0.6077109575271606), ('everyone', 0.6056087017059326), ('mental', 0.6047636270523071), ('mere', 0.6015549898147583), ('witness', 0.5826743245124817)] \n",
      "\n",
      "architect:building::painter:? [('Piraeus', 0.6683154106140137), ('monument', 0.6302096843719482), ('center', 0.6035360097885132), ('road', 0.6011974215507507), ('network', 0.5953367352485657), ('system', 0.5822432041168213), ('seating', 0.5655854940414429), ('accessible', 0.5626335144042969), ('harbour', 0.562239408493042), ('suburb', 0.5622255802154541)] \n",
      "\n",
      "science:physics::planet:? [('theoretical', 0.7611817121505737), ('psychology', 0.749051034450531), ('scientific', 0.7194321155548096), ('mathematics', 0.6650335788726807), ('archaeology', 0.6543601751327515), ('spirituality', 0.641157865524292), ('pacifist', 0.6330896615982056), ('ethics', 0.6230440139770508), ('optics', 0.618963897228241), ('medicine', 0.6175971031188965)] \n",
      "\n",
      "Google:searching::computer:? [('husband', 0.5538033246994019), ('declining', 0.545820415019989), ('disease', 0.5396980047225952), ('Troilus', 0.5343271493911743), ('illnesses', 0.5268625020980835), ('homosexual', 0.52660071849823), ('crimes', 0.5240411758422852), ('perennial', 0.5212334394454956), ('marriage', 0.520577073097229), ('tutor', 0.517112672328949)] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "k= 100\n",
      "man:woman::king:? [('person', 0.5274547338485718), ('young', 0.49952954053878784), ('witness', 0.48073500394821167), ('traveler', 0.4119645357131958), ('creative', 0.391929030418396), ('child', 0.3915501832962036), ('receive', 0.38720422983169556), ('women', 0.3851044476032257), ('everyone', 0.37994569540023804), ('go', 0.37028077244758606)] \n",
      "\n",
      "architect:building::painter:? [('accessible', 0.45595937967300415), ('hub', 0.43671005964279175), ('neighbourhoods', 0.410439670085907), ('housing', 0.40319138765335083), ('built', 0.39380335807800293), ('designed', 0.3937843143939972), ('monument', 0.3821049928665161), ('municipal', 0.3816361427307129), ('system', 0.38106125593185425), ('road', 0.37877753376960754)] \n",
      "\n",
      "science:physics::planet:? [('archaeology', 0.5733174085617065), ('mathematics', 0.5697726011276245), ('psychology', 0.5242676138877869), ('chemistry', 0.49999478459358215), ('analytical', 0.49883130192756653), ('medicine', 0.4866078495979309), ('mysticism', 0.48118120431900024), ('sciences', 0.4800446033477783), ('logic', 0.46428146958351135), ('scientific', 0.4607617259025574)] \n",
      "\n",
      "Google:searching::computer:? [('husband', 0.38801124691963196), ('controversial', 0.36533141136169434), ('Herodotus', 0.3614000678062439), ('murdered', 0.3581153452396393), ('conceived', 0.35589510202407837), ('Martina', 0.35376256704330444), ('investigated', 0.35309189558029175), ('mistaken', 0.35001248121261597), ('claimed', 0.3351666331291199), ('Marcus', 0.3346535563468933)] \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "analogies = [ [['man','woman'],['king','queen']],\\\n",
    "      [['architect','building'],['painter','building']],\\\n",
    "      [['science','physics'],['planet','earth']],\\\n",
    "      [['Google','searching'],['computer','programming']] ]\n",
    "\n",
    "for model in models:\n",
    "    for a in analogies:\n",
    "        for aa in a:\n",
    "            assert aa[0] in model.vocab and aa[1] in model.vocab\n",
    "    \n",
    "for i,model in enumerate(models):\n",
    "    print 'k=',all_k[i]\n",
    "    for a in analogies:\n",
    "        print a[0][0]+':'+a[0][1]+'::'+a[1][0]+':?',model.most_similar(positive = a[0],negative=[a[1][0]]),'\\n'\n",
    "    print '\\n\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Semi-supervised learning #\n",
    "\n",
    "Now you will use the word embeddings to try to improve your dependency parser from problem set 5.\n",
    "\n",
    "You will do this by clustering words according to their embedding.\n",
    "\n",
    "The code below run the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19364186  0.16652703 -0.25888926 ...,  0.11673918  0.14041518\n",
      "  -0.00139467]\n",
      " [-0.01532788  0.10720755 -0.09630613 ...,  0.14305694  0.02934444\n",
      "   0.19374132]\n",
      " [ 0.04454674 -0.03849955 -0.22258388 ...,  0.29765904 -0.03476687\n",
      "  -0.17534447]\n",
      " ..., \n",
      " [-0.0088204  -0.17319834 -0.14212954 ...,  0.21196616 -0.15688029\n",
      "   0.12473287]\n",
      " [-0.14776917 -0.13528498  0.11059188 ...,  0.11769114  0.29699549\n",
      "   0.14274129]\n",
      " [-0.09311409 -0.27388129  0.01792685 ...,  0.13583224  0.0908053\n",
      "   0.10175414]] [ 11  11  11 ..., 192 170   3]\n"
     ]
    }
   ],
   "source": [
    "# choose a model to use here\n",
    "model = models[-1]\n",
    "word_vectors = model.syn0\n",
    "# run the clustering algorithm\n",
    "num_clusters = 200\n",
    "centroids, labels = kmeans2(word_vectors,num_clusters,iter=200,minit='points')\n",
    "print centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 7a** Modify the code below to show the words in the same cluster as a query word. Find the words in the same cluster as \"coffee\", \"computer\", and \"red\" for at least one of your models, plus any other words of interest. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWordsInSameCluster(word,model,labels):\n",
    "    # you will use model.index2word to make this function\n",
    "    # return a list of words in the same cluster as word\n",
    "    words2idx = {word:idx for idx,word in enumerate(model.index2word)}\n",
    "    c = []\n",
    "    idx = words2idx[word]\n",
    "    for i in np.where(labels == labels[idx])[0]:\n",
    "        c.append(model.index2word[i])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animals', 'contain', 'algae', 'almond', 'red', 'almonds', 'trees', 'green', 'diseases', 'wild', 'bacteria', 'meat', 'wine', 'seeds', 'marine', 'toxic', 'sweet', 'cattle', 'dogs', 'milk', 'aromatic', 'lineage', 'domesticated', 'environments', 'sheep', 'horses', 'herbs', 'oils', 'vegetables', 'foods', 'comprise', 'fruits', 'coffee', 'poisonous', 'manure', 'olive', 'corn', 'bread', 'Specific', 'beds', 'dried', 'flour', 'goats', 'brown', 'wool', 'malaria', 'grains', 'honey', 'cows', 'fiber', 'freshwater', 'Algae', 'algal', 'edible', 'shallow', 'potatoes', 'Almonds', 'incense', 'purple', 'gravel', 'mined', 'warmth'] \n",
      "\n",
      "['computer', 'design', 'devices', 'device', 'featured', 'video', 'Newton', 'software', 'digital', 'X', 'setting', 'recognition', 'display', 'computers', 'featuring', 'operating', 'user', 'screen', 'users', 'track', 'alongside', 'online', 'hardware', 'Computer', 'format', 'platform', 'audio', 'cards', 'Atari', 'vector', 'Internet', 'graphics', 'library', 'mobile', 'selling', 'retail', 'Final', 'fans', 'DVD', 'electronics', 'website', 'Microsoft', 'updated', 'purchase', 'phone', 'package', 'firm', 'handwriting', '3D', 'interface', 'option', 'processor', 'AT', 'portable', 'channel', 'upgraded', 'platforms', 'CD', 'Pro', 'wireless', 'tablet', 'Wireless', 'Google', 'delivering'] \n",
      "\n",
      "['animals', 'contain', 'algae', 'almond', 'red', 'almonds', 'trees', 'green', 'diseases', 'wild', 'bacteria', 'meat', 'wine', 'seeds', 'marine', 'toxic', 'sweet', 'cattle', 'dogs', 'milk', 'aromatic', 'lineage', 'domesticated', 'environments', 'sheep', 'horses', 'herbs', 'oils', 'vegetables', 'foods', 'comprise', 'fruits', 'coffee', 'poisonous', 'manure', 'olive', 'corn', 'bread', 'Specific', 'beds', 'dried', 'flour', 'goats', 'brown', 'wool', 'malaria', 'grains', 'honey', 'cows', 'fiber', 'freshwater', 'Algae', 'algal', 'edible', 'shallow', 'potatoes', 'Almonds', 'incense', 'purple', 'gravel', 'mined', 'warmth'] \n",
      "\n",
      "['computer', 'design', 'devices', 'device', 'featured', 'video', 'Newton', 'software', 'digital', 'X', 'setting', 'recognition', 'display', 'computers', 'featuring', 'operating', 'user', 'screen', 'users', 'track', 'alongside', 'online', 'hardware', 'Computer', 'format', 'platform', 'audio', 'cards', 'Atari', 'vector', 'Internet', 'graphics', 'library', 'mobile', 'selling', 'retail', 'Final', 'fans', 'DVD', 'electronics', 'website', 'Microsoft', 'updated', 'purchase', 'phone', 'package', 'firm', 'handwriting', '3D', 'interface', 'option', 'processor', 'AT', 'portable', 'channel', 'upgraded', 'platforms', 'CD', 'Pro', 'wireless', 'tablet', 'Wireless', 'Google', 'delivering'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print getWordsInSameCluster('coffee',model,labels),'\\n'\n",
    "print getWordsInSameCluster('computer',model,labels),'\\n'\n",
    "print getWordsInSameCluster('red',model,labels),'\\n'\n",
    "print getWordsInSameCluster('Google',model,labels),'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving dependency parsing ##\n",
    "\n",
    "Now incorporate these cluster features into your best performing parser from project 5, based on the dev data. Add a feature for each cluster/tag pair, e.g., C175/N, C189/V, etc. You will then compute the accuracy with training sets of various sizes, comparing the performance of your model with and without the cluster features.\n",
    "\n",
    "**Deliverable 7b ** Build training sets including the first 50, 100, 200, 500, and 1000 *sentences* (not words). \n",
    "Train your tagger on each training set, using your original features, and plot the accuracy on the development set. \n",
    "Then retrain you tagger, including the new word cluster features, and plot accuracy on the development set on the same plot. \n",
    "Run for at least 10 iterations in each case.\n",
    "\n",
    "You may want to try larger numbers of clusters to improve performance. \n",
    "\n",
    "You can even include the results from multiple clusterings with different numbers of clusters (50, 100, 200, 500, ...), \n",
    "using features like C43-50/N (cluster 43 of 50, with tag N), C377-500/V (cluster 377 of 500, with tag V).\n",
    "\n",
    "You can also use clusters for the features of neighboring words.  (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "from os.path import join\n",
    "import gtparsing\n",
    "import gtparsing.dependency_parser as depp\n",
    "import gtparsing.dependency_features as depf\n",
    "import gtparsing.custom_features\n",
    "import gtparsing.utilities\n",
    "from score import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtparsing/../data/deppars\n",
      "Number of sentences: 50\n",
      "Number of tokens: 461\n",
      "Number of words: 278\n",
      "Number of pos: 34\n",
      "Number of features: 1647\n",
      "Epoch 1 Train: 0.416 Dev: 0.490\n",
      "Epoch 2 Train: 0.755 Dev: 0.580\n",
      "Epoch 3 Train: 0.861 Dev: 0.608\n",
      "Epoch 4 Train: 0.898 Dev: 0.621\n",
      "Epoch 5 Train: 0.937 Dev: 0.626\n",
      "Epoch 6 Train: 0.961 Dev: 0.629\n",
      "Epoch 7 Train: 0.976 Dev: 0.633\n",
      "Epoch 8 Train: 0.963 Dev: 0.636\n",
      "Epoch 9 Train: 0.991 Dev: 0.636\n",
      "Epoch 10 Train: 0.998 Dev: 0.639\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 100\n",
      "Number of tokens: 975\n",
      "Number of words: 524\n",
      "Number of pos: 37\n",
      "Number of features: 2910\n",
      "Epoch 1 Train: 0.472 Dev: 0.503\n",
      "Epoch 2 Train: 0.785 Dev: 0.563\n",
      "Epoch 3 Train: 0.857 Dev: 0.592\n",
      "Epoch 4 Train: 0.885 Dev: 0.609\n",
      "Epoch 5 Train: 0.933 Dev: 0.611\n",
      "Epoch 6 Train: 0.949 Dev: 0.615\n",
      "Epoch 7 Train: 0.950 Dev: 0.617\n",
      "Epoch 8 Train: 0.968 Dev: 0.623\n",
      "Epoch 9 Train: 0.972 Dev: 0.629\n",
      "Epoch 10 Train: 0.982 Dev: 0.629\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 200\n",
      "Number of tokens: 1873\n",
      "Number of words: 887\n",
      "Number of pos: 41\n",
      "Number of features: 4799\n",
      "Epoch 1 Train: 0.533 Dev: 0.542\n",
      "Epoch 2 Train: 0.779 Dev: 0.582\n",
      "Epoch 3 Train: 0.853 Dev: 0.641\n",
      "Epoch 4 Train: 0.901 Dev: 0.661\n",
      "Epoch 5 Train: 0.916 Dev: 0.669\n",
      "Epoch 6 Train: 0.920 Dev: 0.685\n",
      "Epoch 7 Train: 0.946 Dev: 0.690\n",
      "Epoch 8 Train: 0.960 Dev: 0.684\n",
      "Epoch 9 Train: 0.966 Dev: 0.689\n",
      "Epoch 10 Train: 0.967 Dev: 0.690\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 500\n",
      "Number of tokens: 4933\n",
      "Number of words: 1890\n",
      "Number of pos: 44\n",
      "Number of features: 9899\n",
      "Epoch 1 Train: 0.593 Dev: 0.712\n",
      "Epoch 2 Train: 0.787 Dev: 0.683\n",
      "Epoch 3 Train: 0.842 Dev: 0.741\n",
      "Epoch 4 Train: 0.875 Dev: 0.753\n",
      "Epoch 5 Train: 0.897 Dev: 0.758\n",
      "Epoch 6 Train: 0.930 Dev: 0.764\n",
      "Epoch 7 Train: 0.934 Dev: 0.764\n",
      "Epoch 8 Train: 0.942 Dev: 0.765\n",
      "Epoch 9 Train: 0.948 Dev: 0.765\n",
      "Epoch 10 Train: 0.953 Dev: 0.767\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 1000\n",
      "Number of tokens: 9871\n",
      "Number of words: 3105\n",
      "Number of pos: 45\n",
      "Number of features: 16244\n",
      "Epoch 1 Train: 0.642 Dev: 0.667\n",
      "Epoch 2 Train: 0.772 Dev: 0.704\n",
      "Epoch 3 Train: 0.831 Dev: 0.734\n",
      "Epoch 4 Train: 0.864 Dev: 0.749\n",
      "Epoch 5 Train: 0.887 Dev: 0.751\n",
      "Epoch 6 Train: 0.900 Dev: 0.761\n",
      "Epoch 7 Train: 0.919 Dev: 0.766\n",
      "Epoch 8 Train: 0.923 Dev: 0.771\n",
      "Epoch 9 Train: 0.941 Dev: 0.775\n",
      "Epoch 10 Train: 0.945 Dev: 0.778\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 50\n",
      "Number of tokens: 461\n",
      "Number of words: 278\n",
      "Number of pos: 34\n",
      "Number of features: 1989\n",
      "Epoch 1 Train: 0.416 Dev: 0.475\n",
      "Epoch 2 Train: 0.755 Dev: 0.578\n",
      "Epoch 3 Train: 0.892 Dev: 0.609\n",
      "Epoch 4 Train: 0.952 Dev: 0.617\n",
      "Epoch 5 Train: 0.970 Dev: 0.622\n",
      "Epoch 6 Train: 0.980 Dev: 0.629\n",
      "Epoch 7 Train: 0.987 Dev: 0.629\n",
      "Epoch 8 Train: 0.993 Dev: 0.631\n",
      "Epoch 9 Train: 0.996 Dev: 0.631\n",
      "Epoch 10 Train: 0.996 Dev: 0.632\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 100\n",
      "Number of tokens: 975\n",
      "Number of words: 524\n",
      "Number of pos: 37\n",
      "Number of features: 3558\n",
      "Epoch 1 Train: 0.484 Dev: 0.559\n",
      "Epoch 2 Train: 0.788 Dev: 0.597\n",
      "Epoch 3 Train: 0.871 Dev: 0.621\n",
      "Epoch 4 Train: 0.912 Dev: 0.620\n",
      "Epoch 5 Train: 0.941 Dev: 0.637\n",
      "Epoch 6 Train: 0.941 Dev: 0.641\n",
      "Epoch 7 Train: 0.965 Dev: 0.644\n",
      "Epoch 8 Train: 0.974 Dev: 0.652\n",
      "Epoch 9 Train: 0.967 Dev: 0.657\n",
      "Epoch 10 Train: 0.979 Dev: 0.658\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 200\n",
      "Number of tokens: 1873\n",
      "Number of words: 887\n",
      "Number of pos: 41\n",
      "Number of features: 5893\n",
      "Epoch 1 Train: 0.539 Dev: 0.544\n",
      "Epoch 2 Train: 0.796 Dev: 0.614\n",
      "Epoch 3 Train: 0.874 Dev: 0.651\n",
      "Epoch 4 Train: 0.905 Dev: 0.659\n",
      "Epoch 5 Train: 0.930 Dev: 0.673\n",
      "Epoch 6 Train: 0.962 Dev: 0.670\n",
      "Epoch 7 Train: 0.957 Dev: 0.675\n",
      "Epoch 8 Train: 0.955 Dev: 0.676\n",
      "Epoch 9 Train: 0.961 Dev: 0.680\n",
      "Epoch 10 Train: 0.959 Dev: 0.677\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 500\n",
      "Number of tokens: 4933\n",
      "Number of words: 1890\n",
      "Number of pos: 44\n",
      "Number of features: 12298\n",
      "Epoch 1 Train: 0.597 Dev: 0.663\n",
      "Epoch 2 Train: 0.796 Dev: 0.704\n",
      "Epoch 3 Train: 0.859 Dev: 0.731\n",
      "Epoch 4 Train: 0.895 Dev: 0.747\n",
      "Epoch 5 Train: 0.919 Dev: 0.752\n",
      "Epoch 6 Train: 0.929 Dev: 0.755\n",
      "Epoch 7 Train: 0.950 Dev: 0.757\n",
      "Epoch 8 Train: 0.957 Dev: 0.759\n",
      "Epoch 9 Train: 0.971 Dev: 0.760\n",
      "Epoch 10 Train: 0.971 Dev: 0.760\n",
      "gtparsing/../data/deppars\n",
      "Number of sentences: 1000\n",
      "Number of tokens: 9871\n",
      "Number of words: 3105\n",
      "Number of pos: 45\n",
      "Number of features: 20278\n",
      "Epoch 1 Train: 0.639 Dev: 0.721\n",
      "Epoch 2 Train: 0.787 Dev: 0.746\n",
      "Epoch 3 Train: 0.848 Dev: 0.761\n",
      "Epoch 4 Train: 0.879 Dev: 0.765\n",
      "Epoch 5 Train: 0.906 Dev: 0.765\n",
      "Epoch 6 Train: 0.912 Dev: 0.766\n",
      "Epoch 7 Train: 0.927 Dev: 0.769\n",
      "Epoch 8 Train: 0.939 Dev: 0.772\n",
      "Epoch 9 Train: 0.941 Dev: 0.776\n",
      "Epoch 10 Train: 0.952 Dev: 0.775\n"
     ]
    }
   ],
   "source": [
    "reload(gtparsing)\n",
    "reload(depp)\n",
    "reload(depp)\n",
    "reload(gtparsing)\n",
    "reload(gtparsing.custom_features)\n",
    "reload(gtparsing.dependency_reader)\n",
    "\n",
    "from gtparsing.custom_features import ContextFeats\n",
    "num_insts = [50,100,200,500,1000]\n",
    "\n",
    "dv_accs_cf = []\n",
    "for n_sents in num_insts:\n",
    "    dp = depp.DependencyParser(feature_function=ContextFeats())\n",
    "    dp.read_data(\"english\",n_sents)\n",
    "    tr_acc, dv_acc = dp.train_perceptron(10) \n",
    "    dv_accs_cf.append(dv_acc)\n",
    "\n",
    "dv_accs_w2v = []\n",
    "# choose a model to use here: use the model with word embedding size 100\n",
    "model = models[-1]\n",
    "word_vectors = model.syn0\n",
    "# run the clustering algorithm\n",
    "num_clusters = 500\n",
    "centroids, labels = kmeans2(word_vectors,num_clusters,iter=200,minit='points')\n",
    "for n_sents in num_insts:\n",
    "    words2idx = {word:idx for idx,word in enumerate(model.index2word)}\n",
    "    dp = depp.DependencyParser(feature_function=gtparsing.custom_features.W2V_ClusterFeats(words2idx, labels))\n",
    "    dp.read_data(\"english\",n_sents)\n",
    "    tr_acc ,dv_acc  = dp.train_perceptron(10) \n",
    "    dv_accs_w2v.append(dv_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6387466721277902, 0.6291214417366373, 0.6903542903952489, 0.7669465492525087, 0.7775957403235716]\n",
      "[0.631578947368421, 0.6582019250460782, 0.6770428015564203, 0.7601884087651034, 0.7749334425558059]\n"
     ]
    }
   ],
   "source": [
    "print dv_accs_cf\n",
    "print dv_accs_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fcab1340350>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6Kh18mI9zwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcab11f4110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.plot(num_insts, dv_accs_cf,'o-',label = 'ContextFeats')\n",
    "plt.plot(num_insts,dv_accs_w2v,'o-',label='W2V_ClusterFeats')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('number of sentences')\n",
    "plt.ylabel('dev accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtparsing/../data/deppars\n",
      "Number of sentences: 100\n",
      "Number of tokens: 975\n",
      "Number of words: 524\n",
      "Number of pos: 37\n",
      "Number of features: 3507\n",
      "Epoch 1 Train: 0.481 Dev: 0.514\n",
      "Epoch 2 Train: 0.789 Dev: 0.609\n",
      "Epoch 3 Train: 0.885 Dev: 0.635\n",
      "Epoch 4 Train: 0.920 Dev: 0.643\n",
      "Epoch 5 Train: 0.944 Dev: 0.647\n",
      "Epoch 6 Train: 0.961 Dev: 0.652\n",
      "Epoch 7 Train: 0.966 Dev: 0.656\n",
      "Epoch 8 Train: 0.953 Dev: 0.660\n",
      "Epoch 9 Train: 0.973 Dev: 0.662\n",
      "Epoch 10 Train: 0.979 Dev: 0.665\n"
     ]
    }
   ],
   "source": [
    "# choose a model to use here\n",
    "model = models[-1]\n",
    "word_vectors = model.syn0\n",
    "# run the clustering algorithm\n",
    "num_clusters = 100\n",
    "centroids, labels = kmeans2(word_vectors,num_clusters,iter=100,minit='points')\n",
    "#print centroids, labels\n",
    "\n",
    "words2idx = {word:idx for idx,word in enumerate(model.index2word)}\n",
    "dp = depp.DependencyParser(feature_function=gtparsing.custom_features.W2V_ClusterFeats(words2idx, labels))\n",
    "dp.read_data(\"english\",100)\n",
    "tr_acc ,dv_acc  = dp.train_perceptron(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8. Get creative ##\n",
    "\n",
    "This part is mandatory for 7650 and optional for 4650.\n",
    "\n",
    "**Deliverable 8 ** K-means clustering is one way to find similarity. This part is opened-ended and requires you to come up with creative solutions of using distributed vectors to improve the results. In addition to implementing your idea, provide explanation of what you did, and why you thought it should work.\n",
    "\n",
    "The top 3 selected ideas (based mostly on performance, but also on creativity) will get an extra point towards their final score.(5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### idea: instead of using svd, we can try non-negative matrix factorizatoin\n",
    "- Non-negative matrix factorization is to factorize full matrix $V$ as $V \\approx WH$, where $W$ and $H$ are two low rank matrices with non-negative elements. Since their elements are non-negative, it's more natural to transform them as probability models by normalization. \n",
    "- Here I use the module ```NMF``` in library ```sklearn.decomposition``` \n",
    "- I don't see significant improvement on word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9993x9993 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 8810954 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee ordered complete different perfectly turning scary six dramatic apparently \n",
      "play : play seeing bands animals they on suits order smack ashley \n",
      "crazy : crazy now usually yesterday worked more still i'm will to \n",
      "facebook : facebook anime natural vodka rihanna twerk instagram goodbye 42 pizza \n",
      "hermana : hermana abuela seleccion durmiendo profe clases primera recien huevo fuego \n",
      "computer : computer eating daughter act faced body naked guarantee arm blow \n",
      "google : google eff tom apps perfection jackie delicious goo may wow \n",
      "science : science red rose modern stretch record lewis stephen passes tax \n",
      "math : math mixed childish nonsense ruined dammit complaining quickly bitching screwed \n",
      "physics : physics cocaine beyonce blunts spit ran mexican accent teacher wrist \n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "decomp10 = NMF(n_components = k, init='random',random_state=0)\n",
    "W10 = decomp10.fit_transform(Dfp)\n",
    "printSimilarWords(W10, word_list, sim_func=lambda word_idx, W10 : -computeEuclidDist(word_idx,W10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee : coffee reading watching packed laundry oil fresh bottles burnt tops \n",
      "play : play still her michael star brian curling raise ability rose \n",
      "crazy : crazy gives using drug from kinda attention questions nut thinking \n",
      "facebook : facebook twitter tv train screen turn hooked posts move roller \n",
      "hermana : hermana salir buscar empezar calle llegar durmiendo jamas cabeza pedazo \n",
      "computer : computer loose pictures plate switch moon bite body beside money \n",
      "google : google holy distance z fck handle exist seems material highly \n",
      "science : science runs 2 flying market 1st 5 seconds christmas drawing \n",
      "math : math constant ends target #fb exercise fear comfortable drops sets \n",
      "physics : physics dragon bottle got outfit spend graduated wearing pass panic \n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "decomp50 = NMF(n_components = k, init='random',random_state=0)\n",
    "W50 = decomp50.fit_transform(Dfp)\n",
    "printSimilarWords(W50, word_list, sim_func=lambda word_idx, W50 : -computeEuclidDist(word_idx,W50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
