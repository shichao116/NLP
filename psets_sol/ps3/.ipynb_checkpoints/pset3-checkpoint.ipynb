{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem set 3: Hidden Markov Models\n",
    "=====================\n",
    "\n",
    "- This project focuses on sequence labeling with Hidden Markov models.\n",
    "- The target domain is Twitter part-of-speech tagging\n",
    "- The pset is graded out of 16 points for CS4650, 19 points for CS7650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "%pylab --no-import-all inline\n",
    "import gtnlplib.preproc\n",
    "import gtnlplib.viterbi\n",
    "import gtnlplib.most_common\n",
    "import gtnlplib.naivebayes\n",
    "import gtnlplib.clf_base\n",
    "import gtnlplib.scorer\n",
    "import gtnlplib.constants\n",
    "import gtnlplib.tagger_base\n",
    "import matplotlib.pyplot as plt\n",
    "# this enables you to create inline plots in the notebook \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gtnlplib.naivebayes' from 'gtnlplib/naivebayes.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(gtnlplib.viterbi)\n",
    "reload(gtnlplib.naivebayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing (1 point) # \n",
    "\n",
    "The test data will be released around 48 hours before the deadline.\n",
    "The part-of-speech tags are defined in the [ACL2011 paper](http://www.ark.cs.cmu.edu/TweetNLP/gimpel+etal.acl11.pdf) \n",
    "and the [NAACL 2013 paper](http://www.ark.cs.cmu.edu/TweetNLP/owoputi+etal.naacl13.pdf), \n",
    "which also describe the data and gives some state-of-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define the file names\n",
    "trainfile = gtnlplib.constants.TRAIN_FILE\n",
    "devfile = gtnlplib.constants.DEV_FILE\n",
    "testfile = gtnlplib.constants.TEST_FILE # You do not have this for now\n",
    "offset = gtnlplib.constants.OFFSET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo code for using function \"conllSeqGenerator()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['!', '#', '$', '&', ',', 'A', '@', 'E', 'D', 'G', 'M', 'L', 'O', 'N', 'P', 'S', 'R', 'U', 'T', 'V', 'Y', 'X', 'Z', '^', '~'])\n",
      "['Thankyou', 'for', 'following', 'me', 'today', ',', 'my', 'followers', 'XD'] ['G', 'P', 'V', 'O', 'N', ',', 'D', 'N', 'E']\n"
     ]
    }
   ],
   "source": [
    "## Demo\n",
    "alltags = set()\n",
    "for i,(words, tags) in enumerate(gtnlplib.preproc.conllSeqGenerator(trainfile)):\n",
    "    for tag in tags:\n",
    "        alltags.add(tag)\n",
    "print alltags\n",
    "print words, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 1a** (1 point): Use the Counter class to identify the most common three words for each tag, in the training set. The most_common() function of the Counter class will help you here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! [('lol', 54), ('Lol', 22), ('oh', 10)]\n",
      "# [('#nowplaying', 3), ('#np', 2), ('#tcot', 2)]\n",
      "$ [('one', 32), ('4', 6), ('2010', 6)]\n",
      "& [('and', 117), ('&', 39), ('but', 31)]\n",
      ", [('.', 427), ('!', 244), (',', 225)]\n",
      "A [('good', 24), ('new', 22), ('more', 13)]\n",
      "@ [('@Fresh32Prince89', 6), ('@lil_jeezy_85', 2), ('@ResourcefulMom', 2)]\n",
      "E [(':)', 28), ('<3', 10), (';)', 8)]\n",
      "D [('the', 236), ('a', 165), ('my', 89)]\n",
      "G [('smh', 9), ('|', 7), ('-', 7)]\n",
      "M [(\"momma's\", 1), ('#LebronShould', 1), (\"Ricochet's\", 1)]\n",
      "L [(\"I'm\", 42), ('its', 24), ('im', 15)]\n",
      "O [('I', 258), ('you', 135), ('it', 87)]\n",
      "N [('day', 19), ('time', 18), ('people', 17)]\n",
      "P [('to', 231), ('of', 112), ('for', 101)]\n",
      "S [('mans', 1), (\"judge's\", 1), ('year\\xe2\\x80\\x99s', 1)]\n",
      "R [('just', 56), ('not', 27), ('now', 26)]\n",
      "U [('http', 4), (':/', 1), ('http://blog.tittieflix.com', 1)]\n",
      "T [('out', 29), ('up', 26), ('on', 8)]\n",
      "V [('is', 105), ('are', 52), ('have', 48)]\n",
      "Y [(\"there's\", 2)]\n",
      "X [('all', 6), ('There', 4), ('there', 2)]\n",
      "Z [(\"Obamacare's\", 1), ('\\xe2\\x80\\x9cLasers\\xe2\\x80\\x99s\\xe2\\x80\\x9d', 1), (\"Murkowski's\", 1)]\n",
      "^ [('Heat', 8), ('Halloween', 5), ('twitter', 5)]\n",
      "~ [('RT', 229), (':', 207), ('...', 59)]\n"
     ]
    }
   ],
   "source": [
    "#reload(gtnlplib)\n",
    "counters = gtnlplib.most_common.get_tags(trainfile)\n",
    "for tag,tag_ctr in counters.iteritems():\n",
    "    print tag,tag_ctr.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline models (5 points) # \n",
    "\n",
    "Now you will implement part-of-speech tagging via classification.\n",
    "\n",
    "Tagging quality is evaluated using evalTagger, which takes three arguments:\n",
    "- a tagger, which is a **function** taking a list of words and a tagset as arguments\n",
    "- an output filename\n",
    "- a test file\n",
    "\n",
    "You will want to use lambda expressions to create the first argument for this function, as shown below.\n",
    "Here's how it works. I provide a tagger that labels everything as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.136844287788\n"
     ]
    }
   ],
   "source": [
    "# here is a tagger that just tags everything as a noun\n",
    "noun_tagger = lambda words, alltags : ['N' for word in words]\n",
    "confusion = gtnlplib.tagger_base.evalTagger(noun_tagger,'nouns')\n",
    "print gtnlplib.scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2a ** (1 point)\n",
    "\n",
    "Now do the same thing, but building your tagger *as a classifier.* To do this:\n",
    "\n",
    "- use makeClassifierTagger, which takes as an argument a dict of weights\n",
    "- set the weights yourself, by filling in gtnlplib.most_common.get_noun_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(gtnlplib)\n",
    "cTagger = gtnlplib.tagger_base.makeClassifierTagger(gtnlplib.most_common.get_noun_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.136844287788\n"
     ]
    }
   ],
   "source": [
    "confusion = gtnlplib.tagger_base.evalTagger(cTagger,'nouns')\n",
    "print gtnlplib.scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2b** (2 points)\n",
    "\n",
    "Now build a classifier tagger that tags each word with its most common tag in the training set.\n",
    "\n",
    "- You should again implement your classifier by defining a set of weights\n",
    "- Prediction should use your predict() function from pset 1. (you are allowed to edit this function if you don't think you got it right in pset 1.)\n",
    "- For unseen words, the classifier should choose the tag with the most **unique** word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631349782293\n"
     ]
    }
   ],
   "source": [
    "weights = gtnlplib.most_common.get_most_common_weights(gtnlplib.constants.TRAIN_FILE)\n",
    "confusion = gtnlplib.tagger_base.evalTagger(gtnlplib.tagger_base.makeClassifierTagger(weights),'mcc')\n",
    "print gtnlplib.scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2c** (1 point)\n",
    "\n",
    "Now use your function ```learnNBWeights``` from pset 2 to set the weights in the classifier-tagger.\n",
    "\n",
    "You will need feature-class counts (where there is one feature per tag: the word), and class counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a list of all words\n",
    "allwords = set()\n",
    "for counts in counters.values():\n",
    "    allwords.update(set(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_counts = gtnlplib.most_common.get_class_counts(counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.73472314667 -12.1116332442 -9.39015620613 -4.61138281324 -3.58372417191\n"
     ]
    }
   ],
   "source": [
    "w1 = gtnlplib.naivebayes.learnNBWeights(counters,class_counts,allwords)\n",
    "print w1[('N','breakfast')], w1[('V','breakfast')], w1[('A','smart')], w1[('D','the')], w1[('!',gtnlplib.constants.OFFSET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2d** (1 point): run the code below to evaluate your naive bayes tagger on the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 0.629069044163\n",
      "0.001 0.629069044163\n",
      "0.01 0.629069044163\n",
      "0.1 0.626580966204\n",
      "1.0 0.583661621397\n",
      "10.0 0.468588015758\n"
     ]
    }
   ],
   "source": [
    "dev_acc = dict()\n",
    "for alpha in [1e-4,1e-3,1e-2,1e-1,1e0,1e1]:\n",
    "    nb_weights = gtnlplib.naivebayes.learnNBWeights(counters,class_counts,allwords,alpha)\n",
    "    confusion = gtnlplib.tagger_base.evalTagger(gtnlplib.tagger_base.makeClassifierTagger(nb_weights),'nb')\n",
    "    dev_acc[alpha] = gtnlplib.scorer.accuracy(confusion)\n",
    "    print alpha,dev_acc[alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Viterbi Algorithm (10 points) #\n",
    "\n",
    "In this section you will implement the Viterbi algorithm. As a reminder, here it is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\vec{w}'\\vec{f}(\\vec{x},\\vec{y}) = & \\sum_i \\vec{w}^{\\top} \\vec{f}(\\vec{x},y_i,y_{i-1},i) \\\\\n",
    "v(y,0) = & \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,\\diamond,0)\\\\\n",
    "b(y,0) = & \\diamond \\\\\n",
    "v(y,i) = & \\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
    "b(y,i-1) = & \\text{arg}\\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get warmed up, let's work out an example by hand. These are only two tags, \n",
    "N and V. Here are the parameters:\n",
    "\n",
    "| | Value |\n",
    "| ------------- |:-------------:|\n",
    "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
    "| $\\log P_E(\\cdot|V)$ | they: -10, can: -2, fish: -3 |\n",
    "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
    "| $\\log P_T(\\cdot|V)$ | N: -1, V: -4, END: -3 |\n",
    "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-1 |\n",
    "\n",
    "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the translation probability.\n",
    " \n",
    "In class we discuss the sentence *They can fish*. Now work out a more complicated example: \"*They can can fish*\".\n",
    " \n",
    "** Deliverable 3a ** (2 points) Show the trellis-like table, and give the score for the best best scoring path(s). After you work out the trellis by hand, you should be able to fill the following table.\n",
    "\n",
    "\n",
    "** Sanity check ** There are two paths that each score -18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Fill your answer in the following table)*\n",
    "\n",
    "|POS tag| START  | they | can | can | fish | END |\n",
    "|-------|:-------|:-----|:----|:----|:-----|:---:|\n",
    "| N     |    0   |  -2  | -10 | -10 | -16  | -18 |\n",
    "| V     |    0   |  -11 | -6 | -12| -15  | -18 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Viterbi ##\n",
    "\n",
    "Here are some predefined weights, corresponding to problem 3a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_tag = gtnlplib.constants.START_TAG\n",
    "trans = gtnlplib.constants.TRANS\n",
    "end_tag = gtnlplib.constants.END_TAG\n",
    "emit = gtnlplib.constants.EMIT\n",
    "\n",
    "defined_weights = {('N','they',emit):-1,('N','can',emit):-3,('N','fish',emit):-3,\\\n",
    "                        ('V','they',emit):-10,('V','can',emit):-2,('V','fish',emit):-3,\\\n",
    "                        ('N','N',trans):-5,('V','N',trans):-2,(end_tag,'N',trans):-3,\\\n",
    "                        ('N','V',trans):-1,('V','V',trans):-4,(end_tag,'V',trans):-3,\\\n",
    "                        ('N',start_tag,trans):-1,('V',start_tag,trans):-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainfile = gtnlplib.constants.TRAIN_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gtnlplib.viterbi.hmm_feats` computes the HMM features for the function $\\vec{f}(\\vec{x},y,y',i)$. \n",
    "- You will call it in your viterbi tagger. \n",
    "- Note that it returns both an emission and transition feature, except for the last word, where it returns only a transition feature. \n",
    "- Also note that transition and emission features are specially marked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3b** (5 points)\n",
    "\n",
    "Implement `viterbiTagger` in `gtnlplib/viterbi.py`\n",
    "\n",
    "- **Input 1**: a list of words\n",
    "- **Input 2**: a feature function, like hmm_feats\n",
    "- **Input 3**: a dict of weights\n",
    "- **Input 4**: a list of all possible tags\n",
    "- **Output 1**: the best-scoring sequence\n",
    "- **Output 2**: the score of the best-scoring sequence\n",
    "\n",
    "Hint: you can represent the trellis and the back pointers as lists of dicts. You will want to do some special handling for the first and last words; otherwise, just iterate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run you viterbi tagger on the example in 3a, using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmm_feats(words,curr_tag,prev_tag,i):\n",
    "    \"\"\"Feature function for HMM that returns emit and transition features\"\"\"\n",
    "    if i < len(words):\n",
    "        return [(curr_tag,words[i],emit),(curr_tag,prev_tag,trans)]\n",
    "    else:\n",
    "        return [(curr_tag,prev_tag,trans)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['N', 'V', 'N', 'V'], -18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtnlplib.viterbi.viterbiTagger(['they','can','can','fish'],hmm_feats,defined_weights,['N','V'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your Viterbi on the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = 'they can can can can can can can fish'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['N', 'V', 'N', 'V', 'N', 'V', 'N', 'V', 'N'], -37)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtnlplib.viterbi.viterbiTagger(sent,hmm_feats,defined_weights,['N','V'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3c**\n",
    "(2 points)\n",
    "\n",
    "Now estimate the weights of a hidden Markov model. \n",
    "- You have already estimated the emission weights $\\log P(w | y)$, in your solution to problem 2. Use your solution with $\\alpha=0.001$\n",
    "- Estimate the transition probabilities from the training data, using the maximum-likelihood estimates (no smoothing)\n",
    "- Don't forget transitions from the start state and to the end state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gtnlplib.naivebayes' from 'gtnlplib/naivebayes.pyc'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(gtnlplib.viterbi)\n",
    "reload(gtnlplib.naivebayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gtnlplib/viterbi.py:112: RuntimeWarning: divide by zero encountered in log\n",
      "  trans_weights[(tag,t_tag)] = np.log(float(trans_counter[(tag,t_tag)])/count)\n",
      "gtnlplib/viterbi.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "  trans_weights[(tag,END_TAG)] = np.log(float(trans_counter[(tag,END_TAG)])/num_insts)\n",
      "gtnlplib/viterbi.py:113: RuntimeWarning: divide by zero encountered in log\n",
      "  trans_weights[(START_TAG, tag)] = np.log(float(trans_counter[(START_TAG, tag)])/num_insts)\n"
     ]
    }
   ],
   "source": [
    "hmm_weights = gtnlplib.viterbi.get_HMM_weights(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.75977511074 -12.5906306787 -1000.0\n",
      "-1.91791454157 -3.343419581\n",
      "-1000.0 -1000.0\n"
     ]
    }
   ],
   "source": [
    "print hmm_weights['V','go',emit], hmm_weights['~','go',emit], hmm_weights['^','diddy',emit]\n",
    "print hmm_weights['V','V',trans], hmm_weights['~','V',trans]\n",
    "print hmm_weights[end_tag,'V',trans], hmm_weights[end_tag,'~',trans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**: here's the tag sequence and score for our example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['V', 'O', 'V', 'V', 'N', 'D'], -2060.9341909738737)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtnlplib.viterbi.viterbiTagger([':))','we','can','can','fish',':-)'],hmm_feats,hmm_weights,alltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3d** (1 point):\n",
    "- Run your HMM tagger on the dev data, using the code line below.\n",
    "- ** Sanity check**: I get 74.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (['O', 'V', 'O', 'V', 'N', 'D', 'P', 'N', 'O', 'V', 'P', ',', 'V', '^', '^', 'N', ',', 'R', 'P', 'O', 'V', 'L', 'P', 'O', '~', '@', ',', '^', ',', '~', ',', 'N'], -2310.7772819360262)\n",
      "1 (['~', '@', '~', ',', 'N', 'V', 'N', 'D', 'P', 'N', ','], -2117.6066967549241)\n",
      "2 (['^', '^', '^', '$', ',', 'N', 'D', 'P', 'N', 'E'], -2107.7417725596833)\n"
     ]
    }
   ],
   "source": [
    "for i,(words,_) in enumerate(gtnlplib.preproc.conllSeqGenerator(trainfile)):\n",
    "    print i, gtnlplib.viterbi.viterbiTagger(words,hmm_feats,hmm_weights,alltags)\n",
    "    if i >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584905660377\n"
     ]
    }
   ],
   "source": [
    "confusion = gtnlplib.tagger_base.evalTagger(lambda words, alltags : gtnlplib.viterbi.viterbiTagger(words,hmm_feats,hmm_weights,alltags)[0],'hmm')\n",
    "print gtnlplib.scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. 7650-only (3 points) #\n",
    "\n",
    "**Deliverable 4a** (3 points)\n",
    "\n",
    "Find an example of expectation-maximization in use in a paper at ACL, NAACL, EMNLP, EACL, or TACL, within the last five years (2010-2015). List:\n",
    "\n",
    "- The title, authors, and venue of the paper\n",
    "- What is the \"missing data\" (latent variable) that they are imputing in the E-step?\n",
    "- What are the parameters that they are trying to estimate?\n",
    "- Do they take any steps to correct for local optima?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(your response here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
